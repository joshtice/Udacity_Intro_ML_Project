{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# Udacity module imports\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier\n",
    "\n",
    "# Scikit-Learn imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Imbalanced-Learn imports\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Select what features you'll use.\n",
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\".\n",
    "\n",
    "feature_list = [\n",
    "    'poi',  \n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    #'email_address',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person',\n",
    "    'from_this_person_to_poi',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages',\n",
    "    'total_payments',\n",
    "    'total_stock_value'\n",
    "]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 97343619,\n",
       " 'deferral_payments': 32083396,\n",
       " 'deferred_income': -27992891,\n",
       " 'director_fees': 1398517,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'expenses': 5235198,\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 83925000,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'other': 42667589,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 130322299,\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'salary': 26704229,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 309886585,\n",
       " 'total_stock_value': 434509511}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data into dataframe for EDA\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "df.set_index(employees, inplace=True)\n",
    "df.replace('NaN', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "#print('Shape: {}'.format(df.shape))\n",
    "#print(df.describe())\n",
    "#sns.boxplot(x='poi', y='bonus', data=df)\n",
    "sns.boxplot(x='poi', y='to_messages', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 3: Create new feature(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['bonus', 'exercised_stock_options', 'expenses', 'loan_advances',\n",
      "       'long_term_incentive', 'other', 'salary', 'shared_receipt_with_poi',\n",
      "       'total_payments', 'total_stock_value'],\n",
      "      dtype='|S25')\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Try a varity of classifiers\n",
    "# Please name your classifier clf for easy export below.\n",
    "# Note that if you want to do PCA or other multi-stage operations,\n",
    "# you'll need to use Pipelines. For more info:\n",
    "# http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "dataset = featureFormat(data_dict, feature_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(dataset)\n",
    "\n",
    "# Scale features \n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "# Perform additional features selection\n",
    "feature_selection = SelectKBest(score_func=chi2, k=10)\n",
    "feature_selection.fit(X_train, y_train)\n",
    "pprint.pprint(np.array(feature_list)[1:][feature_selection.get_support()])\n",
    "X_train = feature_selection.transform(X_train)\n",
    "X_test = feature_selection.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create first classifier\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=GaussianNB(), \n",
    "    n_estimators=300, \n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test first classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create second classifier\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "X_train, y_train = upsampler.fit_sample(X_train, y_train)\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    n_estimators=300, \n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test second classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "13.0\n",
      "(171, 10)\n",
      "84.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-poi       0.97      0.77      0.86        39\n",
      "        poi       0.31      0.80      0.44         5\n",
      "\n",
      "avg / total       0.89      0.77      0.81        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create second classifier\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "print(X_train.shape)\n",
    "print(sum(y_train))\n",
    "X_train, y_train = upsampler.fit_sample(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(sum(y_train))\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    n_estimators=300, \n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test second classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-poi       0.94      0.77      0.85        39\n",
      "        poi       0.25      0.60      0.35         5\n",
      "\n",
      "avg / total       0.86      0.75      0.79        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refresh\n",
    "del(dataset, labels, features, X_train, X_test, y_train, y_test, y_pred,\n",
    "    scaler, upsampler, feature_selection, classifier, pipe, scorer, param_grid, clf)\n",
    "\n",
    "\n",
    "# Start with data from scratch\n",
    "dataset = featureFormat(data_dict, feature_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(dataset)\n",
    "\n",
    "# Scale features before splitting into train/test sets\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Upsample the 'poi' class using the ADASYN algorithm\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "X_train, y_train = upsampler.fit_sample(X_train, y_train)\n",
    "\n",
    "# Define pipeline\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    random_state=42\n",
    ")\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Tune classifier with grid search \n",
    "scorer = make_scorer(fbeta_score, beta=0.5, average='weighted')\n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection': [PCA()],\n",
    "        'feature_selection__n_components': [2, 5, 10, 15]\n",
    "    },\n",
    "    {\n",
    "        'feature_selection': [SelectKBest(score_func=chi2)],\n",
    "        'feature_selection__k': [2, 5, 10, 15], \n",
    "        'classifier__n_estimators': [200, 300, 400, 1000]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=scorer)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))\n",
    "#pprint.pprint(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6092f8eef5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdump_classifier_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/joshuatice/Documents/Udacity/IntroML/ud120-projects/final_project/tester.pyc\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_classifier_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m### Run testing script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuatice/Documents/Udacity/IntroML/ud120-projects/final_project/tester.pyc\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(clf, dataset, feature_list, folds)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuatice/Documents/Udacity/IntroML/ud120-projects/tools/feature_format.pyc\u001b[0m in \u001b[0;36mfeatureFormat\u001b[0;34m(dictionary, features, remove_NaN, remove_all_zeroes, remove_any_zeroes, sort_keys)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(clf, dataset, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create second classifier\n",
    "scaler = MinMaxScaler()\n",
    "feature_selection = SelectKBest(score_func=chi2, k=5)\n",
    "classifier = AdaBoostClassifier(base_estimator=GaussianNB(), n_estimators=300, random_state=42)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADASYN',\n",
      " 'AdaBoostClassifier',\n",
      " 'DecisionTreeClassifier',\n",
      " 'GaussianNB',\n",
      " 'GridSearchCV',\n",
      " 'In',\n",
      " 'MinMaxScaler',\n",
      " 'Out',\n",
      " 'PCA',\n",
      " 'Pipeline',\n",
      " 'RandomForestClassifier',\n",
      " 'SVC',\n",
      " 'SelectFromModel',\n",
      " 'SelectKBest',\n",
      " 'X_test',\n",
      " 'X_train',\n",
      " '_',\n",
      " '_4',\n",
      " '__',\n",
      " '___',\n",
      " '__builtin__',\n",
      " '__builtins__',\n",
      " '__doc__',\n",
      " '__name__',\n",
      " '__package__',\n",
      " '_dh',\n",
      " '_i',\n",
      " '_i1',\n",
      " '_i2',\n",
      " '_i3',\n",
      " '_i4',\n",
      " '_i5',\n",
      " '_i6',\n",
      " '_i7',\n",
      " '_i8',\n",
      " '_i9',\n",
      " '_ih',\n",
      " '_ii',\n",
      " '_iii',\n",
      " '_oh',\n",
      " '_sh',\n",
      " 'chi2',\n",
      " 'classification_report',\n",
      " 'classifier',\n",
      " 'clf',\n",
      " 'data_dict',\n",
      " 'data_file',\n",
      " 'dataset',\n",
      " 'dump_classifier_and_data',\n",
      " 'exit',\n",
      " 'f1_score',\n",
      " 'fbeta_score',\n",
      " 'featureFormat',\n",
      " 'feature_list',\n",
      " 'feature_selection',\n",
      " 'features',\n",
      " 'get_ipython',\n",
      " 'labels',\n",
      " 'make_scorer',\n",
      " 'np',\n",
      " 'param_grid',\n",
      " 'pd',\n",
      " 'pickle',\n",
      " 'pipe',\n",
      " 'plt',\n",
      " 'pprint',\n",
      " 'quit',\n",
      " 'scaler',\n",
      " 'scorer',\n",
      " 'sns',\n",
      " 'sys',\n",
      " 'targetFeatureSplit',\n",
      " 'test_classifier',\n",
      " 'train_test_split',\n",
      " 'upsampler',\n",
      " 'y_pred',\n",
      " 'y_test',\n",
      " 'y_train']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "# using our testing script. Check the tester.py script in the final project\n",
    "# folder for details on the evaluation method, especially the test_classifier\n",
    "# function. Because of the small size of the dataset, the script uses\n",
    "# stratified shuffle split cross validation. For more info:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "#test_classifier(clf, data_dict, feature_list, folds = 1000)\n",
    "\n",
    "# Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "# check your results. You do not need to change anything below, but make sure\n",
    "# that the version of poi_id.py that you submit can be run on its own and\n",
    "# generates the necessary .pkl files for validating your results.\n",
    "\n",
    "# dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    #test_multiple(classifier_types, features_train, features_test, labels_train,\n",
    "    #labels_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
