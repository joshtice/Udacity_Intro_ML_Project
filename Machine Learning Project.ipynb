{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Fraud from Enron Email and Financial Data \n",
    "Joshua Tice\n",
    "## Introduction  \n",
    "The goal of this project was to use a supervised machine learning algorithm to identify Enron employees who committed fraud based on the public Enron Email and Financial Dataset. Machine learning was a particularly useful tool in identifying persons of interest because its algorithms could identify patterns and relationships in high-dimensional data that were difficult for a human mind to discern.  \n",
    "## Exploratory data analysis\n",
    "Before discussing the exploratory analysis of the dataset, I loaded a series of libraries and modules that were used throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# General imports\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# Udacity module imports\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier\n",
    "\n",
    "# Scikit-Learn imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Imbalanced-Learn imports\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curated dataset was provided by Udacity as a pickle file, which was subsequently loaded into a Python dictionary with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ported the dictionary into a Pandas dataframe so that I could perform a preliminary exploratory data analysis. I started by viewing the features that were available as well as a sample of the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, METTS MARK to GLISAN JR BEN F\n",
      "Data columns (total 21 columns):\n",
      "bonus                        146 non-null object\n",
      "deferral_payments            146 non-null object\n",
      "deferred_income              146 non-null object\n",
      "director_fees                146 non-null object\n",
      "email_address                146 non-null object\n",
      "exercised_stock_options      146 non-null object\n",
      "expenses                     146 non-null object\n",
      "from_messages                146 non-null object\n",
      "from_poi_to_this_person      146 non-null object\n",
      "from_this_person_to_poi      146 non-null object\n",
      "loan_advances                146 non-null object\n",
      "long_term_incentive          146 non-null object\n",
      "other                        146 non-null object\n",
      "poi                          146 non-null bool\n",
      "restricted_stock             146 non-null object\n",
      "restricted_stock_deferred    146 non-null object\n",
      "salary                       146 non-null object\n",
      "shared_receipt_with_poi      146 non-null object\n",
      "to_messages                  146 non-null object\n",
      "total_payments               146 non-null object\n",
      "total_stock_value            146 non-null object\n",
      "dtypes: bool(1), object(20)\n",
      "memory usage: 24.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>METTS MARK</th>\n",
       "      <td>600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mark.metts@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94299</td>\n",
       "      <td>29</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1740</td>\n",
       "      <td>False</td>\n",
       "      <td>585062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365788</td>\n",
       "      <td>702</td>\n",
       "      <td>807</td>\n",
       "      <td>1061827</td>\n",
       "      <td>585062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELLIOTT STEVEN</th>\n",
       "      <td>350000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-400729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>steven.elliott@enron.com</td>\n",
       "      <td>4890344</td>\n",
       "      <td>78552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12961</td>\n",
       "      <td>False</td>\n",
       "      <td>1788391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211725</td>\n",
       "      <td>6678735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CORDES WILLIAM R</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bill.cordes@enron.com</td>\n",
       "      <td>651850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>386335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1038185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANNON KEVIN P</th>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3117011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kevin.hannon@enron.com</td>\n",
       "      <td>5538001</td>\n",
       "      <td>34039</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>1617011</td>\n",
       "      <td>11350</td>\n",
       "      <td>True</td>\n",
       "      <td>853064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243293</td>\n",
       "      <td>1035</td>\n",
       "      <td>1045</td>\n",
       "      <td>288682</td>\n",
       "      <td>6391065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bonus deferral_payments deferred_income director_fees  \\\n",
       "METTS MARK         600000               NaN             NaN           NaN   \n",
       "BAXTER JOHN C     1200000           1295738        -1386055           NaN   \n",
       "ELLIOTT STEVEN     350000               NaN         -400729           NaN   \n",
       "CORDES WILLIAM R      NaN               NaN             NaN           NaN   \n",
       "HANNON KEVIN P    1500000               NaN        -3117011           NaN   \n",
       "\n",
       "                             email_address exercised_stock_options expenses  \\\n",
       "METTS MARK            mark.metts@enron.com                     NaN    94299   \n",
       "BAXTER JOHN C                          NaN                 6680544    11200   \n",
       "ELLIOTT STEVEN    steven.elliott@enron.com                 4890344    78552   \n",
       "CORDES WILLIAM R     bill.cordes@enron.com                  651850      NaN   \n",
       "HANNON KEVIN P      kevin.hannon@enron.com                 5538001    34039   \n",
       "\n",
       "                 from_messages from_poi_to_this_person  \\\n",
       "METTS MARK                  29                      38   \n",
       "BAXTER JOHN C              NaN                     NaN   \n",
       "ELLIOTT STEVEN             NaN                     NaN   \n",
       "CORDES WILLIAM R            12                      10   \n",
       "HANNON KEVIN P              32                      32   \n",
       "\n",
       "                 from_this_person_to_poi        ...         \\\n",
       "METTS MARK                             1        ...          \n",
       "BAXTER JOHN C                        NaN        ...          \n",
       "ELLIOTT STEVEN                       NaN        ...          \n",
       "CORDES WILLIAM R                       0        ...          \n",
       "HANNON KEVIN P                        21        ...          \n",
       "\n",
       "                 long_term_incentive    other    poi  restricted_stock  \\\n",
       "METTS MARK                       NaN     1740  False            585062   \n",
       "BAXTER JOHN C                1586055  2660303  False           3942714   \n",
       "ELLIOTT STEVEN                   NaN    12961  False           1788391   \n",
       "CORDES WILLIAM R                 NaN      NaN  False            386335   \n",
       "HANNON KEVIN P               1617011    11350   True            853064   \n",
       "\n",
       "                 restricted_stock_deferred  salary shared_receipt_with_poi  \\\n",
       "METTS MARK                             NaN  365788                     702   \n",
       "BAXTER JOHN C                          NaN  267102                     NaN   \n",
       "ELLIOTT STEVEN                         NaN  170941                     NaN   \n",
       "CORDES WILLIAM R                       NaN     NaN                      58   \n",
       "HANNON KEVIN P                         NaN  243293                    1035   \n",
       "\n",
       "                 to_messages total_payments total_stock_value  \n",
       "METTS MARK               807        1061827            585062  \n",
       "BAXTER JOHN C            NaN        5634343          10623258  \n",
       "ELLIOTT STEVEN           NaN         211725           6678735  \n",
       "CORDES WILLIAM R         764            NaN           1038185  \n",
       "HANNON KEVIN P          1045         288682           6391065  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data into dataframe for EDA\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "df.set_index(employees, inplace=True)\n",
    "\n",
    "# Basic EDA\n",
    "# Inspect features\n",
    "print(df.info())\n",
    "# Sample data at the head of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saw that most of the features contained mixed datatypes due to the presence of 'NaN' strings included for missing values. I subsequently replaced these values with the numpy respresentation for null values and re-printed the info for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, METTS MARK to GLISAN JR BEN F\n",
      "Data columns (total 21 columns):\n",
      "bonus                        82 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "email_address                111 non-null object\n",
      "exercised_stock_options      102 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "other                        93 non-null float64\n",
      "poi                          146 non-null bool\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "salary                       95 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "dtypes: bool(1), float64(19), object(1)\n",
      "memory usage: 24.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Replace 'NaN' strings with Numpy nan representation\n",
    "df.replace('NaN', np.nan, inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data had a mixture of financial features, features related to emails, and a classification. The 14 financial features were reported in units of US dollars:  \n",
    "* bonus\n",
    "* deferral payments\n",
    "* deferred income\n",
    "* director fees\n",
    "* exercised stock options\n",
    "* expenses\n",
    "* loan advances\n",
    "* long term incentive\n",
    "* other\n",
    "* restricted stock\n",
    "* restricted stock deferred\n",
    "* salary\n",
    "* total payments\n",
    "* total stock value  \n",
    "\n",
    "The 6 features related to emails were reported in units of counts (except email address, which was a string): \n",
    "\n",
    "* email address\n",
    "* from messages\n",
    "* from poi to this person\n",
    "* from this person to poi\n",
    "* shared receipt with poi\n",
    "* to messages  \n",
    "\n",
    "The classification feature (poi) was a boolean.\n",
    "\n",
    "I was interested in the allocation between the two different classes (poi and non-poi), because the efficacy of the learning algorithm could have been affected negatively by inbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points labelled 'POI': 18\n",
      "Data points labelled 'non-POI': 128\n",
      "Fraction of data points labelled 'POI': 0.12\n"
     ]
    }
   ],
   "source": [
    "# What is the allocation across classes (non-POI and POI)?\n",
    "count_poi = sum(df.poi)\n",
    "count_non_poi = sum(np.array(df.poi) == False)\n",
    "fraction_poi = count_poi / (count_poi + count_non_poi)\n",
    "print(\"Data points labelled 'POI': {}\".format(count_poi))\n",
    "print(\"Data points labelled 'non-POI': {}\".format(count_non_poi))\n",
    "print(\"Fraction of data points labelled 'POI': {:.2}\".format(fraction_poi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that the poi class composed only 12% of the data, I decided to investigate an up-sampling strategy later to equally populate both classes.  \n",
    "\n",
    "Many of the features had a significant fraction of missing values. I re-sorted the features based on the number of 'nan's and also plotted a histogram to visualize the distribution of missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature | Number of missing values\n",
      "[('loan_advances', 142),\n",
      " ('director_fees', 129),\n",
      " ('restricted_stock_deferred', 128),\n",
      " ('deferral_payments', 107),\n",
      " ('deferred_income', 97),\n",
      " ('long_term_incentive', 80),\n",
      " ('bonus', 64),\n",
      " ('to_messages', 60),\n",
      " ('from_poi_to_this_person', 60),\n",
      " ('from_messages', 60),\n",
      " ('from_this_person_to_poi', 60),\n",
      " ('shared_receipt_with_poi', 60),\n",
      " ('other', 53),\n",
      " ('salary', 51),\n",
      " ('expenses', 51),\n",
      " ('exercised_stock_options', 44),\n",
      " ('restricted_stock', 36),\n",
      " ('email_address', 35),\n",
      " ('total_payments', 21),\n",
      " ('total_stock_value', 20),\n",
      " ('poi', 0)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFXCAYAAACYx4YhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMP8P8LpRQyS51hqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11079be10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Investigate missing values\n",
    "missing_value_dict = {}\n",
    "for col in df.columns:\n",
    "    missing_value_dict[col] = sum(df[col].isnull())\n",
    "print(\"Feature | Number of missing values\")\n",
    "pprint.pprint(sorted(missing_value_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "sns.distplot(missing_value_dict.values(), bins=range(0, 150, 10), kde=False)\n",
    "plt.xlabel('Number of missing values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial feature selection\n",
    "\n",
    "In some data analyses, a prevalance of missing values might have been a legitimate reason to remove a feature before implementing a learning algorithm. However, in this analysis, I knew that many persons of interest held positions at the top of the organization. Hence, features that might not have been relevant for an average employee (*e.g.*, director fees) might have been highly relevant to employees in upper management and consequently highly useful for the machine learning algorithm. Consequently, I did not remove any features based on missing values. In the above exploratory analysis, I represented missing values as 'nan's. However, when applying the learning algorithm, it made logical sense to represent the missing values as 0 USD or 0 counts. Code below provided by Udacity made this transformation during the data import process for machine learning.\n",
    "\n",
    "I did remove one feature at this stage - email address. I believed that the the email address of each individual was unlikely to contain any information relevant for machine learning. This feature was essentially a unique string identifier for each data point, and basically a redundant index, but with missing values for some data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Select what features you'll use.\n",
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\".\n",
    "\n",
    "feature_list = [\n",
    "    'poi',  \n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    #'email_address',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person',\n",
    "    'from_this_person_to_poi',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages',\n",
    "    'total_payments',\n",
    "    'total_stock_value'\n",
    "]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "\n",
    "Based on the \"Outliers\" lesson from Udacity's \"Intro to Machine Learning\" course, I knew that the data contained one outlier that needed to be removed - a data point corresponding to the cumulative total for each features. I did not remove any other outliers from the data because in this particular machine learning exercise, I was purposefully trying to identify individuals that could be seen as anomalies. In all likelihood, many of the outliers probably provided the most useful information for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 2: Remove outliers\n",
    "del(data_dict['TOTAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of new features\n",
    "\n",
    "I created two new features from the supplied data. The first, 'ratio_poi_from_messages', was the number of emails sent from persons of interest to an individual normalized by the total number of emails the individual received. I reasoned that if a person received a high volume of emails in general (*e.g.*, an administrative assistant), then that person might also have a high count of emails from poi's by coincidence. The fraction of emails from poi might be a better indicator of affiliation with a poi. Likewise, I normalized the number of emails each individual sent to poi's by the total number of emails sent ('ratio_poi_to_messages')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 3: Create new feature(s)\n",
    "for key in data_dict.keys():\n",
    "    if (data_dict[key]['from_poi_to_this_person'] == 'NaN') or \\\n",
    "       (data_dict[key]['from_messages'] == 'NaN'):\n",
    "        data_dict[key]['ratio_poi_from_messages'] = 'NaN'\n",
    "    else:\n",
    "        data_dict[key]['ratio_poi_from_messages'] = \\\n",
    "        data_dict[key]['from_poi_to_this_person'] / \\\n",
    "        data_dict[key]['from_messages']\n",
    "    if (data_dict[key]['from_this_person_to_poi'] == 'NaN') or \\\n",
    "       (data_dict[key]['to_messages'] == 'NaN'):\n",
    "        data_dict[key]['ratio_poi_to_messages'] = 'NaN'\n",
    "    else:\n",
    "        data_dict[key]['ratio_poi_to_messages'] = \\\n",
    "        data_dict[key]['from_this_person_to_poi'] / \\\n",
    "        data_dict[key]['to_messages']\n",
    "feature_list.append('ratio_poi_from_messages')\n",
    "feature_list.append('ratio_poi_to_messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "Some of the features in the dataset had drastically different ranges. For instance, the features that I created above were purposefully created on a scale of 0 to 1, while some of the provided financial features had values in the millions of dollars. While not all learning algorithms require feature scaling, some could have substantially biased by non-normalized data, such as SVM's or linear models. To make sure all the features had similar ranges, I applied a simple min/max scaler to the data. At this point, the data were transferred to a Numpy array to be compatible with SciKit-Learn's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features and labels from dataset for local testing\n",
    "dataset = featureFormat(data_dict, feature_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(dataset)\n",
    "\n",
    "# Scale features \n",
    "#scaler = MinMaxScaler()\n",
    "#features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'learning_rate': 0.1}\n",
      "Best f1 score 0.21\n",
      "Precision: 0.350574462574\n",
      "Recall: 0.21\n",
      "Precision / Recall\n",
      "[(0.0, 0.0),\n",
      " (0.25143265993265995, 0.10222222222222223),\n",
      " (0.35057446257446256, 0.20999999999999999)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (GRADIENT BOOST TUNE)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "# Define classifier\n",
    "boost = GradientBoostingClassifier( \n",
    "    loss='deviance', \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=100, \n",
    "    subsample=1.0, \n",
    "    criterion='friedman_mse', \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_depth=3, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, \n",
    "    init=None, \n",
    "    random_state=42, \n",
    "    max_features=None, \n",
    "    verbose=0, \n",
    "    max_leaf_nodes=None, \n",
    "    warm_start=False, \n",
    "    presort='auto'\n",
    ")\n",
    "\n",
    "# Tune classifier with grid search \n",
    "# Ref: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "param_grid = [\n",
    "    {\n",
    "        #'n_estimators': [],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(boost, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='recall')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "print(\"Best f1 score {:.2}\".format(clf.best_score_))\n",
    "print(\"Precision: {}\".format(clf.cv_results_['mean_test_precision'][clf.best_index_]))\n",
    "print(\"Recall: {}\".format(clf.cv_results_['mean_test_recall'][clf.best_index_]))\n",
    "print(\"Precision / Recall\")\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "\tAccuracy: 0.85260\tPrecision: 0.39962\tRecall: 0.21000\tF1: 0.27532\tF2: 0.23202\n",
      "\tTotal predictions: 15000\tTrue positives:  420\tFalse positives:  631\tFalse negatives: 1580\tTrue negatives: 12369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_test = clf.best_estimator_\n",
    "test_classifier(clf_test, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'n_estimators': 3, 'learning_rate': 0.01, 'base_estimator__max_depth': 10}\n",
      "Best f1 score 0.25\n",
      "Precision: 0.319527176527\n",
      "Recall: 0.248888888889\n",
      "Precision / Recall\n",
      "[(0.3096375661375661, 0.22444444444444445),\n",
      " (0.31470923520923522, 0.18777777777777777),\n",
      " (0.29903571428571429, 0.1438888888888889),\n",
      " (0.3082059884559884, 0.21555555555555556),\n",
      " (0.29474494949494945, 0.17166666666666666),\n",
      " (0.2804206349206349, 0.12666666666666668),\n",
      " (0.33556587856587849, 0.21666666666666667),\n",
      " (0.32690343915343917, 0.16388888888888889),\n",
      " (0.28717328042328044, 0.13444444444444445),\n",
      " (0.32220153920153921, 0.22777777777777777),\n",
      " (0.32672667147667145, 0.20166666666666666),\n",
      " (0.32366354016354015, 0.17999999999999999),\n",
      " (0.32710293410293412, 0.22555555555555556),\n",
      " (0.33337614237614238, 0.20499999999999999),\n",
      " (0.32053090428090425, 0.17722222222222223),\n",
      " (0.29072005772005771, 0.20166666666666666),\n",
      " (0.29409175084175082, 0.18555555555555556),\n",
      " (0.29280206830206834, 0.15833333333333333),\n",
      " (0.32216979316979316, 0.20944444444444443),\n",
      " (0.31935101010101008, 0.20000000000000001),\n",
      " (0.31550180375180376, 0.19388888888888889),\n",
      " (0.33307239057239058, 0.21777777777777776),\n",
      " (0.32625048100048099, 0.20833333333333334),\n",
      " (0.3303893698893699, 0.20444444444444446),\n",
      " (0.30321989121989124, 0.20000000000000001),\n",
      " (0.31465031265031262, 0.19555555555555557),\n",
      " (0.31270971620971622, 0.19166666666666668),\n",
      " (0.32530543530543532, 0.22666666666666666),\n",
      " (0.326759139009139, 0.22611111111111112),\n",
      " (0.326759139009139, 0.22555555555555556),\n",
      " (0.32185569985569989, 0.22833333333333333),\n",
      " (0.32066257816257815, 0.22611111111111112),\n",
      " (0.32471813371813374, 0.22722222222222221),\n",
      " (0.3146295093795094, 0.22944444444444445),\n",
      " (0.3136295093795094, 0.22777777777777777),\n",
      " (0.31423268398268395, 0.22722222222222221),\n",
      " (0.32167845117845117, 0.24055555555555555),\n",
      " (0.32101178451178447, 0.23999999999999999),\n",
      " (0.32101178451178447, 0.23999999999999999),\n",
      " (0.3265210437710438, 0.24666666666666667),\n",
      " (0.32585437710437709, 0.24611111111111111),\n",
      " (0.32585437710437709, 0.24611111111111111),\n",
      " (0.30669035594035593, 0.2361111111111111),\n",
      " (0.30669035594035593, 0.23555555555555555),\n",
      " (0.30585702260702263, 0.23499999999999999),\n",
      " (0.32382082732082729, 0.24833333333333332),\n",
      " (0.32382082732082729, 0.24833333333333332),\n",
      " (0.32382082732082729, 0.24833333333333332),\n",
      " (0.31952717652717649, 0.24888888888888888),\n",
      " (0.31952717652717649, 0.24888888888888888),\n",
      " (0.31952717652717649, 0.24888888888888888),\n",
      " (0.30523352573352575, 0.24388888888888888),\n",
      " (0.30523352573352575, 0.24388888888888888),\n",
      " (0.30523352573352575, 0.24388888888888888)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (ADABOOST TUNE)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "# Define classifier\n",
    "boost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(\n",
    "        criterion='gini', \n",
    "        splitter='best',\n",
    "        max_depth=None, # Tune this guy\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features= 'sqrt',\n",
    "        random_state=42,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        min_impurity_split=None,\n",
    "        class_weight='balanced',\n",
    "        presort=False\n",
    "    ), \n",
    "    n_estimators=50, \n",
    "    learning_rate=1.0, \n",
    "    algorithm='SAMME.R', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tune classifier with grid search \n",
    "# Ref: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "param_grid = [\n",
    "    {\n",
    "        #'n_estimators': [10, 20, 30],\n",
    "        #'learning_rate': [0.001, 0.01, 0.1],\n",
    "        #'base_estimator__max_depth': range(11, 16),\n",
    "        #'base_estimator__max_features': [None, 'sqrt']\n",
    "        'n_estimators': [3, 5, 8],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'base_estimator__max_depth': range(5, 11),\n",
    "        \n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(boost, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='recall')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "print(\"Best f1 score {:.2}\".format(clf.best_score_))\n",
    "print(\"Precision: {}\".format(clf.cv_results_['mean_test_precision'][clf.best_index_]))\n",
    "print(\"Recall: {}\".format(clf.cv_results_['mean_test_recall'][clf.best_index_]))\n",
    "print(\"Precision / Recall\")\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.01, n_estimators=3, random_state=42)\n",
      "\tAccuracy: 0.83453\tPrecision: 0.34019\tRecall: 0.25650\tF1: 0.29247\tF2: 0.26977\n",
      "\tTotal predictions: 15000\tTrue positives:  513\tFalse positives:  995\tFalse negatives: 1487\tTrue negatives: 12005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_test = clf.best_estimator_\n",
    "test_classifier(clf_test, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'C': 1}\n",
      "Best f1 score 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Precision / Recall\n",
      "[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (SUPPORT VECTOR MACHINE)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "class_weight = {\n",
    "    0: 1,\n",
    "    1: 1000\n",
    "}\n",
    "# Define classifier\n",
    "svc = SVC(\n",
    "    C=1.0, \n",
    "    kernel='rbf', \n",
    "    degree=3, \n",
    "    gamma='auto', \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=class_weight, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape='ovr', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tune classifier with grid search \n",
    "# Ref: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': [1, 10, 100, 500, 1000], \n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(svc, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='f1')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "print(\"Best f1 score {:.2}\".format(clf.best_score_))\n",
    "print(\"Precision: {}\".format(clf.cv_results_['mean_test_precision'][clf.best_index_]))\n",
    "print(\"Recall: {}\".format(clf.cv_results_['mean_test_recall'][clf.best_index_]))\n",
    "print(\"Precision / Recall\")\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_test = clf.best_estimator_\n",
    "test_classifier(clf_test, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'C': 100000000.0}\n",
      "Best f1 score 0.28\n",
      "Precision: 0.281533022819\n",
      "Recall: 0.538333333333\n",
      "Precision / Recall\n",
      "[(0.13860793616177169, 0.99944444444444447),\n",
      " (0.19870053368124041, 0.91166666666666663),\n",
      " (0.2239545621750188, 0.72944444444444445),\n",
      " (0.24341213770923692, 0.65333333333333332),\n",
      " (0.25371364456251383, 0.63277777777777777),\n",
      " (0.28153302281940051, 0.53833333333333333)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (LOGISITIC REGRESSION)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "class_weight = {\n",
    "    0: 1,\n",
    "    1: 1000\n",
    "}\n",
    "# Define classifier\n",
    "lm = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    dual=False, \n",
    "    tol=0.0001, \n",
    "    C=1.0, \n",
    "    fit_intercept=True, \n",
    "    intercept_scaling=1, \n",
    "    class_weight=class_weight, \n",
    "    random_state=42, \n",
    "    solver='liblinear', \n",
    "    max_iter=100, \n",
    "    multi_class='ovr', \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Tune classifier with grid search \n",
    "# Ref: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': [1, 10, 100, 500, 1000], \n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(lm, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='precision')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "print(\"Best f1 score {:.2}\".format(clf.best_score_))\n",
    "print(\"Precision: {}\".format(clf.cv_results_['mean_test_precision'][clf.best_index_]))\n",
    "print(\"Recall: {}\".format(clf.cv_results_['mean_test_recall'][clf.best_index_]))\n",
    "print(\"Precision / Recall\")\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000000.0, class_weight={0: 1, 1: 1000}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.41787\tPrecision: 0.16883\tRecall: 0.85800\tF1: 0.28214\tF2: 0.47236\n",
      "\tTotal predictions: 15000\tTrue positives: 1716\tFalse positives: 8448\tFalse negatives:  284\tTrue negatives: 4552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_test = clf.best_estimator_\n",
    "test_classifier(clf_test, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'classifier__class_weight': {0: 1, 1: 10}, 'classifier__C': 1000, 'feature_selection__k': 9}\n",
      "[(0.23068496396578761, 0.5411111111111111),\n",
      " (0.23527869559306955, 0.59777777777777774),\n",
      " (0.22367762351039294, 0.57166666666666666),\n",
      " (0.21647027485475009, 0.55333333333333334),\n",
      " (0.21093544195624739, 0.53833333333333333),\n",
      " (0.21084282681636232, 0.50388888888888894),\n",
      " (0.21067878871801599, 0.46722222222222221),\n",
      " (0.17526095960531424, 0.80500000000000005),\n",
      " (0.20213082942885804, 0.73277777777777775),\n",
      " (0.20097543758083447, 0.67055555555555557),\n",
      " (0.20268295661207195, 0.65111111111111108),\n",
      " (0.1941110634607548, 0.60277777777777775),\n",
      " (0.19989498713236137, 0.58333333333333337),\n",
      " (0.21078038748391154, 0.54888888888888887),\n",
      " (0.15150850798963708, 0.8783333333333333),\n",
      " (0.17387900778798288, 0.80055555555555558),\n",
      " (0.18180338542997368, 0.72666666666666668),\n",
      " (0.19081975462097617, 0.68999999999999995),\n",
      " (0.19225119577085473, 0.64166666666666672),\n",
      " (0.19563749664771937, 0.59999999999999998),\n",
      " (0.20634067714469551, 0.55777777777777782)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (SCALER -> FEATURE SELECTION -> SVC)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='micro', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "class_weight = {\n",
    "    0: 1,\n",
    "    1: 100\n",
    "}\n",
    "scaler = MinMaxScaler()\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = SVC(\n",
    "    C=1.0, \n",
    "    kernel='rbf', \n",
    "    degree=3, \n",
    "    gamma='auto', \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=None, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape='ovr', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Tune classifier with grid search \n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection__k': range(3, 10),\n",
    "        'classifier__C': [1000],\n",
    "        'classifier__class_weight': [{0:1, 1:10}, {0:1, 1:20}, {0:1, 1:50}]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='f1')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('feature_selection', SelectKBest(k=10, score_func=<function chi2 at 0x1101b86e0>)), ('classifier', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "clf_test = pipe\n",
    "test_classifier(clf_test, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'classifier__class_weight': {0: 1, 1: 10}, 'classifier__C': 1, 'feature_selection__k': 9}\n",
      "Best f1 score 0.24\n",
      "Precision: 0.238565869137\n",
      "Recall: 0.693333333333\n",
      "Precision / Recall\n",
      "[(0.19766681033667047, 0.69666666666666666),\n",
      " (0.21718971937861567, 0.68611111111111112),\n",
      " (0.22585822591979568, 0.67277777777777781),\n",
      " (0.2299916023890238, 0.68333333333333335),\n",
      " (0.23465558331166847, 0.69166666666666665),\n",
      " (0.2360569704708867, 0.69166666666666665),\n",
      " (0.23856586913698682, 0.69333333333333336),\n",
      " (0.13162815839229824, 0.98277777777777775),\n",
      " (0.1424600856211212, 0.97611111111111115),\n",
      " (0.15632667540394948, 0.96388888888888891),\n",
      " (0.16969786337871856, 0.96499999999999997),\n",
      " (0.18330720061660055, 0.96388888888888891),\n",
      " (0.1935265210333007, 0.95777777777777773),\n",
      " (0.19728496999074738, 0.93888888888888888),\n",
      " (0.12584901921665037, 0.99888888888888894),\n",
      " (0.12651508865419553, 0.99833333333333329),\n",
      " (0.12939282321485671, 0.99777777777777776),\n",
      " (0.13332500730754449, 0.995),\n",
      " (0.13730387975019614, 0.9916666666666667),\n",
      " (0.14629625364440413, 0.9916666666666667),\n",
      " (0.15421450623303512, 0.98555555555555552)]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (SCALER -> FEATURE SELECTION -> LOGISTIC REGRESSION)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "scaler = MinMaxScaler()\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    dual=False, \n",
    "    tol=0.0001, \n",
    "    C=1.0, \n",
    "    fit_intercept=True, \n",
    "    intercept_scaling=1, \n",
    "    class_weight='balanced', \n",
    "    random_state=42, \n",
    "    solver='liblinear', \n",
    "    max_iter=100, \n",
    "    multi_class='ovr', \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Tune classifier with grid search \n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection__k': range(3, 10),\n",
    "        'classifier__C': [1],\n",
    "        'classifier__class_weight': [{0:1, 1:10}, {0:1, 1:20}, {0:1, 1:50}]        \n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='precision')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('feature_selection', SelectKBest(k=10, score_func=<function chi2 at 0x1101b86e0>)), ('classifier', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.73233\tPrecision: 0.25179\tRecall: 0.51100\tF1: 0.33735\tF2: 0.42375\n",
      "\tTotal predictions: 15000\tTrue positives: 1022\tFalse positives: 3037\tFalse negatives:  978\tTrue negatives: 9963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_test = pipe\n",
    "test_classifier(clf_test, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adasyn_mod(ADASYN):\n",
    "    def __init__(self, ratio='auto', random_state=None, k=None, n_neighbors=5, n_jobs=1):\n",
    "        ADASYN.__init__(self, ratio='auto', random_state=None, k=None, n_neighbors=5, n_jobs=1)\n",
    "        \n",
    "    def transform(self):\n",
    "        return ADASYN.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform. 'ADASYN(k=None, n_jobs=1, n_neighbors=5, random_state=42, ratio='minority')' (type <class 'imblearn.over_sampling.adasyn.ADASYN'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-305-b885359b308f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'upsampler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'feature_selection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m ])\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuatice/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuatice/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 raise TypeError(\"All intermediate steps should be \"\n\u001b[1;32m    161\u001b[0m                                 \u001b[0;34m\"transformers and implement fit and transform.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                 \" '%s' (type %s) doesn't\" % (t, type(t)))\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# We allow last estimator to be None as an identity transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform. 'ADASYN(k=None, n_jobs=1, n_neighbors=5, random_state=42, ratio='minority')' (type <class 'imblearn.over_sampling.adasyn.ADASYN'>) doesn't"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (SVC WITH FEATURE SELECTION AND UPSAMPLING)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "class_weight = {\n",
    "    0: 1,\n",
    "    1: 100\n",
    "}\n",
    "\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = SVC(\n",
    "    C=1.0, \n",
    "    kernel='rbf', \n",
    "    degree=3, \n",
    "    gamma='auto', \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight='balanced', \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape='ovr', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('upsampler', upsampler),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Tune classifier with grid search \n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection__k': [5, 10, 15],\n",
    "        'classifier__C': [1, 10, 100]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='f1')\n",
    "clf.fit(features, labels)\n",
    "print(\"Best parameters {}\".format(clf.best_params_))\n",
    "print(\"Best f1 score {:.2}\".format(clf.best_score_))\n",
    "print(\"Precision: {}\".format(clf.cv_results_['mean_test_precision'][clf.best_index_]))\n",
    "print(\"Recall: {}\".format(clf.cv_results_['mean_test_recall'][clf.best_index_]))\n",
    "print(\"Precision / Recall\")\n",
    "pprint.pprint(zip(clf.cv_results_['mean_test_precision'], clf.cv_results_['mean_test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean precision: [ 0.12844433  0.22148259  0.24235572  0.25066117  0.25316391]\n",
      "Mean recall: [ 1.          0.86444444  0.59333333  0.47111111  0.42944444]\n",
      "Feature weights:\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean precision: {}\".format(clf.cv_results_['mean_test_precision']))\n",
    "print(\"Mean recall: {}\".format(clf.cv_results_['mean_test_recall']))\n",
    "print(\"Feature weights:\")\n",
    "#ranked_features = dict(zip(np.array(feature_list)[1:], clf.best_estimator_.feature_importances_))\n",
    "#pprint.pprint(sorted(ranked_features.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=13, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42)\n",
      "\tAccuracy: 0.83013\tPrecision: 0.33593\tRecall: 0.28050\tF1: 0.30572\tF2: 0.29007\n",
      "\tTotal predictions: 15000\tTrue positives:  561\tFalse positives: 1109\tFalse negatives: 1439\tTrue negatives: 11891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(\n",
    "        criterion='gini', \n",
    "        splitter='best',\n",
    "        # Tune\n",
    "        max_depth=13, \n",
    "        # Tune?\n",
    "        min_samples_split=2,\n",
    "        # Imbalanced class problem - keep min_samples_leaf low\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        min_impurity_split=None,\n",
    "        class_weight='balanced',\n",
    "        presort=False\n",
    "    ), \n",
    "    random_state=42\n",
    ")\n",
    "test_classifier(clf, data_dict, feature_list=feature_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('other', 6.9274771054647131),\n",
      " ('ratio_poi_from_messages', 6.7427476116836855),\n",
      " ('bonus', 5.1933492599908826),\n",
      " ('salary', 1.525656574023559),\n",
      " ('long_term_incentive', 1.4892171166615551),\n",
      " ('total_payments', 1.3985123944024678),\n",
      " ('total_stock_value', 1.019639207181404),\n",
      " ('loan_advances', 0.34172579360229516),\n",
      " ('shared_receipt_with_poi', 0.066521993376217045),\n",
      " ('exercised_stock_options', 0.058560878398035338)]\n"
     ]
    }
   ],
   "source": [
    "# Perform additional features selection\n",
    "feature_selection = SelectKBest(score_func=chi2, k=10)\n",
    "feature_selection.fit(features, labels)\n",
    "mask = feature_selection.get_support()\n",
    "ranks = dict(zip(np.array(feature_list)[1:][mask], feature_selection.scores_))\n",
    "pprint.pprint(sorted(ranks.items(), key=lambda x: x[1], reverse=True))\n",
    "X = feature_selection.transform(features)\n",
    "# Up sample features to account for class imbalance\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "X, y = upsampler.fit_sample(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision: 0.27\n",
      "Precision 95% confidence interval: [-0.09, 0.63]\n",
      "Average recall: 0.27\n",
      "Recall 95% confidence interval: [-0.092, 0.63]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (CLASSIFIERS AND CV)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer\n",
    "}\n",
    "# Classifier 1\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    n_estimators=300, \n",
    "    random_state=42\n",
    ")\n",
    "# Classifier 2\n",
    "clf2 = SVC(C=100, random_state=42)\n",
    "# Compute validation score statistics\n",
    "scores = cross_validate(clf, features, labels, cv=cv, scoring=scoring_dict, return_train_score=False)\n",
    "precision_mean = scores['test_precision'].mean()\n",
    "precision_std = scores['test_precision'].std()\n",
    "recall_mean = scores['test_recall'].mean()\n",
    "recall_std = scores['test_recall'].std()\n",
    "print(\"Average precision: {:.2}\".format(precision_mean))\n",
    "print(\"Precision 95% confidence interval: [{:.2}, {:.2}]\"\\\n",
    "      .format(precision_mean - 1.96*precision_std, precision_mean + 1.96*precision_std))\n",
    "print(\"Average recall: {:.2}\".format(recall_mean))\n",
    "print(\"Recall 95% confidence interval: [{:.2}, {:.2}]\"\\\n",
    "      .format(recall_mean - 1.96*recall_std, recall_mean + 1.96*recall_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=300, random_state=42)\n",
      "\tAccuracy: 0.80787\tPrecision: 0.26911\tRecall: 0.25700\tF1: 0.26292\tF2: 0.25933\n",
      "\tTotal predictions: 15000\tTrue positives:  514\tFalse positives: 1396\tFalse negatives: 1486\tTrue negatives: 11604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf, data_dict, feature_list=['poi']+ranks.keys(), folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_estimators': 100,\n",
      " 'feature_selection': SelectKBest(k=5, score_func=<function chi2 at 0x1101b86e0>),\n",
      " 'feature_selection__k': 5,\n",
      " 'feature_selection__score_func': <function chi2 at 0x1101b86e0>}\n"
     ]
    }
   ],
   "source": [
    "# TESTING TESTING (GRIDSEARCH)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1': f1_scorer\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    random_state=42\n",
    "\n",
    ")\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Tune classifier with grid search \n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection': [PCA()],\n",
    "        'feature_selection__n_components': [5, 10, 15],\n",
    "        'classifier__n_estimators': [100, 300, 500]\n",
    "    },\n",
    "    {\n",
    "        'feature_selection': [SelectKBest()],\n",
    "        'feature_selection__score_func': [f_classif, chi2],\n",
    "        'feature_selection__k': [5, 10, 15], \n",
    "        'classifier__n_estimators': [100, 300, 500]\n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring_dict, cv=cv, refit='f1')\n",
    "clf.fit(features, labels)\n",
    "pprint.pprint(clf.best_params_)\n",
    "#test_classifier(clf, data_dict, feature_list=['poi']+ranks.keys(), folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': <sklearn.model_selection._split.RepeatedStratifiedKFold at 0x1107c4610>,\n",
       " 'error_score': 'raise',\n",
       " 'estimator': Pipeline(memory=None,\n",
       "      steps=[('feature_selection', SelectKBest(k=10, score_func=<function chi2 at 0x1101b86e0>)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "             max_depth=None, max_features=None, max_leaf_nodes=...None,\n",
       "             splitter='best'),\n",
       "           learning_rate=1.0, n_estimators=50, random_state=42))]),\n",
       " 'estimator__classifier': AdaBoostClassifier(algorithm='SAMME.R',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "             max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "           learning_rate=1.0, n_estimators=50, random_state=42),\n",
       " 'estimator__classifier__algorithm': 'SAMME.R',\n",
       " 'estimator__classifier__base_estimator': DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "             max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " 'estimator__classifier__base_estimator__class_weight': 'balanced',\n",
       " 'estimator__classifier__base_estimator__criterion': 'gini',\n",
       " 'estimator__classifier__base_estimator__max_depth': None,\n",
       " 'estimator__classifier__base_estimator__max_features': None,\n",
       " 'estimator__classifier__base_estimator__max_leaf_nodes': None,\n",
       " 'estimator__classifier__base_estimator__min_impurity_decrease': 0.0,\n",
       " 'estimator__classifier__base_estimator__min_impurity_split': None,\n",
       " 'estimator__classifier__base_estimator__min_samples_leaf': 1,\n",
       " 'estimator__classifier__base_estimator__min_samples_split': 2,\n",
       " 'estimator__classifier__base_estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'estimator__classifier__base_estimator__presort': False,\n",
       " 'estimator__classifier__base_estimator__random_state': None,\n",
       " 'estimator__classifier__base_estimator__splitter': 'best',\n",
       " 'estimator__classifier__learning_rate': 1.0,\n",
       " 'estimator__classifier__n_estimators': 50,\n",
       " 'estimator__classifier__random_state': 42,\n",
       " 'estimator__feature_selection': SelectKBest(k=10, score_func=<function chi2 at 0x1101b86e0>),\n",
       " 'estimator__feature_selection__k': 10,\n",
       " 'estimator__feature_selection__score_func': <function sklearn.feature_selection.univariate_selection.chi2>,\n",
       " 'estimator__memory': None,\n",
       " 'estimator__steps': [('feature_selection',\n",
       "   SelectKBest(k=10, score_func=<function chi2 at 0x1101b86e0>)),\n",
       "  ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
       "             base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "               max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "             learning_rate=1.0, n_estimators=50, random_state=42))],\n",
       " 'fit_params': None,\n",
       " 'iid': True,\n",
       " 'n_jobs': 1,\n",
       " 'param_grid': [{'classifier__n_estimators': [100, 300, 500],\n",
       "   'feature_selection': [PCA(copy=True, iterated_power='auto', n_components=15, random_state=None,\n",
       "      svd_solver='auto', tol=0.0, whiten=False)],\n",
       "   'feature_selection__n_components': [5, 10, 15]},\n",
       "  {'classifier__n_estimators': [100, 300, 500],\n",
       "   'feature_selection': [SelectKBest(k=5, score_func=<function chi2 at 0x1101b86e0>)],\n",
       "   'feature_selection__k': [5, 10, 15],\n",
       "   'feature_selection__score_func': [<function sklearn.feature_selection.univariate_selection.f_classif>,\n",
       "    <function sklearn.feature_selection.univariate_selection.chi2>]}],\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': 'f1',\n",
       " 'return_train_score': 'warn',\n",
       " 'scoring': {'f1': make_scorer(f1_score, average=binary, pos_label=1),\n",
       "  'precision': make_scorer(precision_score, average=binary, pos_label=1),\n",
       "  'recall': make_scorer(recall_score, average=binary, pos_label=1)},\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.301832010582\n",
      "0.266666666667\n",
      "0.270121797048\n"
     ]
    }
   ],
   "source": [
    "print(clf.cv_results_['mean_test_precision'].max())\n",
    "print(clf.cv_results_['mean_test_recall'].max())\n",
    "print(clf.cv_results_['mean_test_f1'].max())\n",
    "pprint.pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feature_selection', SelectKBest(k=10, score_func=<function chi2 at 0x1101b86e0>)), ('classifier', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'feature_selection': [PCA(copy=True, iterated_power='auto', n_components=15, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)], 'feature_selection__n_components': [2, 5, 10, 15], 'classifier__C': [10, 100, 1000]}, {'feature_selection__k': [2, 5, 10, 15], 'feature_selection...ction f_classif at 0x1101b85f0>, <function chi2 at 0x1101b86e0>], 'classifier__C': [10, 100, 1000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
       "       scoring={'recall': make_scorer(recall_score, average=binary, pos_label=1), 'precision': make_scorer(precision_score, average=binary, pos_label=1)},\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING TESTING (GRIDSEARCH)\n",
    "\n",
    "# Define validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=100, random_state=42)\n",
    "# Scoring methods\n",
    "precision_scorer = make_scorer(precision_score, average='binary', pos_label=1)\n",
    "recall_scorer = make_scorer(recall_score, average='binary', pos_label=1)\n",
    "scoring_dict = {\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = SVC(C=1000, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Tune classifier with grid search \n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection': [PCA()],\n",
    "        'feature_selection__n_components': [2, 5, 10, 15],\n",
    "        'classifier__C': [10, 100, 1000]\n",
    "    },\n",
    "    {\n",
    "        'feature_selection': [SelectKBest()],\n",
    "        'feature_selection__score_func': [f_classif, chi2],\n",
    "        'feature_selection__k': [2, 5, 10, 15], \n",
    "        'classifier__C': [10, 100, 1000]\n",
    "    }\n",
    "]\n",
    "clf2 = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring_dict, refit=False)\n",
    "clf2.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61111111111111116\n",
      "0.22222222222222221\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(clf2.cv_results_['mean_test_precision'].max())\n",
    "pprint.pprint(clf2.cv_results_['mean_test_recall'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation strategy\n",
    "\n",
    "If the learning algorithm was trained on the entire dataset, then the likelihood of overfitting would have been exacerbated. I would have no way of assessing the effecivenes of the learning algorithm when presented with previously unseen data. Hence, after scaling all of the data, I split the data into a training set and a test set for validation purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-48aa1d224b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform additional features selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeature_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform additional features selection\n",
    "feature_selection = SelectKBest(score_func=chi2, k=10)\n",
    "feature_selection.fit(X_train, y_train)\n",
    "mask = feature_selection.get_support()\n",
    "ranks = dict(zip(np.array(feature_list)[1:][mask], feature_selection.scores_))\n",
    "pprint.pprint(sorted(ranks.items(), key=lambda x: x[1], reverse=True))\n",
    "X_train = feature_selection.transform(X_train)\n",
    "X_test = feature_selection.transform(X_test)\n",
    "\n",
    "# Up sample features to account for class imbalance\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "X_train, y_train = upsampler.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-poi       0.92      0.90      0.91        39\n",
      "        poi       0.33      0.40      0.36         5\n",
      "\n",
      "avg / total       0.85      0.84      0.85        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Try a varity of classifiers\n",
    "# Please name your classifier clf for easy export below.\n",
    "# Note that if you want to do PCA or other multi-stage operations,\n",
    "# you'll need to use Pipelines. For more info:\n",
    "# http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Create first classifier\n",
    "clf = SVC(C=1, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test first classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-poi       0.97      0.77      0.86        39\n",
      "        poi       0.31      0.80      0.44         5\n",
      "\n",
      "avg / total       0.89      0.77      0.81        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create second classifier\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    n_estimators=300, \n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test second classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-poi       0.94      0.77      0.85        39\n",
      "        poi       0.25      0.60      0.35         5\n",
      "\n",
      "avg / total       0.86      0.75      0.79        44\n",
      "\n",
      "{'classifier__n_estimators': 200,\n",
      " 'feature_selection': SelectKBest(k=10, score_func=<function chi2 at 0x1171c66e0>),\n",
      " 'feature_selection__k': 10,\n",
      " 'feature_selection__score_func': <function chi2 at 0x1171c66e0>}\n"
     ]
    }
   ],
   "source": [
    "# Refresh# Define pipeline\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    random_state=42\n",
    ")\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "del(dataset, labels, features, X_train, X_test, y_train, y_test, y_pred,\n",
    "    scaler, upsampler, feature_selection, classifier, pipe, scorer, param_grid, clf)\n",
    "\n",
    "\n",
    "# Start with data from scratch\n",
    "dataset = featureFormat(data_dict, feature_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(dataset)\n",
    "\n",
    "# Scale features before splitting into train/test sets\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Upsample the 'poi' class using the ADASYN algorithm\n",
    "upsampler = ADASYN(ratio='minority', random_state=42)\n",
    "X_train, y_train = upsampler.fit_sample(X_train, y_train)\n",
    "\n",
    "# Define pipeline\n",
    "feature_selection = SelectKBest(score_func=chi2)\n",
    "classifier = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(class_weight='balanced'), \n",
    "    random_state=42\n",
    ")\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Make scorer\n",
    "scorer = make_scorer(fbeta_score, beta=0.1, average='binary', pos_label=1)\n",
    "\n",
    "# Tune classifier with grid search \n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection': [PCA()],\n",
    "        'feature_selection__n_components': [2, 5, 10, 15]\n",
    "    },\n",
    "    {\n",
    "        'feature_selection': [SelectKBest()],\n",
    "        'feature_selection__score_func': [f_classif, chi2],\n",
    "        'feature_selection__k': [2, 5, 10, 15], \n",
    "        'classifier__n_estimators': [200, 300, 400]\n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=scorer)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['non-poi', 'poi']))\n",
    "pprint.pprint(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6092f8eef5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdump_classifier_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/joshuatice/Documents/Udacity/IntroML/ud120-projects/final_project/tester.pyc\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_classifier_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m### Run testing script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuatice/Documents/Udacity/IntroML/ud120-projects/final_project/tester.pyc\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(clf, dataset, feature_list, folds)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuatice/Documents/Udacity/IntroML/ud120-projects/tools/feature_format.pyc\u001b[0m in \u001b[0;36mfeatureFormat\u001b[0;34m(dictionary, features, remove_NaN, remove_all_zeroes, remove_any_zeroes, sort_keys)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "#dump_classifier_and_data(clf, dataset, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "# using our testing script. Check the tester.py script in the final project\n",
    "# folder for details on the evaluation method, especially the test_classifier\n",
    "# function. Because of the small size of the dataset, the script uses\n",
    "# stratified shuffle split cross validation. For more info:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "test_classifier(clf, data_dict, feature_list, folds = 1000)\n",
    "\n",
    "# Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "# check your results. You do not need to change anything below, but make sure\n",
    "# that the version of poi_id.py that you submit can be run on its own and\n",
    "# generates the necessary .pkl files for validating your results.\n",
    "\n",
    "# dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    #test_multiple(classifier_types, features_train, features_test, labels_train,\n",
    "    #labels_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
